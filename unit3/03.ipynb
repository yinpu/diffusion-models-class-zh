{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24dc5801",
   "metadata": {},
   "source": [
    "# Stable Diffusion 深度剖析\n",
    "\n",
    "Stable Diffusion 是一款强大的文本生成图像模型。市面上有多种网站和工具可帮助你轻松使用它。该模型也已[集成进 Hugging Face 的 diffusers 库](https://huggingface.co/blog/stable_diffusion)，只需几行代码即可生成图像：\n",
    "\n",
    "```python\n",
    "from diffusers import StableDiffusionPipeline\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-2-1-base\",\n",
    "    variant=\"fp16\",\n",
    "    torch_dtype=torch.float16,\n",
    ").to(\"cuda\")\n",
    "image = pipe(\"An astronaut scuba diving\").images[0]\n",
    "```\n",
    "\n",
    "在本笔记本中，我们将深入剖析这些易用接口背后的代码，了解其内部工作原理。首先，我们会把上面的简洁示例“拆解”成一段看似可怕的大块代码；接着，我们将逐一检视不同的组件，弄清它们的职责与作用。等到笔记本结束时，你应该能够随心所欲地调整与改写。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91faef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from base64 import b64encode\n",
    "\n",
    "import numpy\n",
    "import torch\n",
    "from diffusers import AutoencoderKL, LMSDiscreteScheduler, UNet2DConditionModel\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# For video display:\n",
    "from IPython.display import HTML\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torch import autocast\n",
    "from torchvision import transforms as tfms\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, logging\n",
    "import os\n",
    "\n",
    "torch.manual_seed(1)\n",
    "if not (Path.home()/'.cache/huggingface'/'token').exists(): notebook_login()\n",
    "\n",
    "# 忽略加载 CLIPTextModel 时的多余警告\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# 根据硬件环境选择设备：优先使用 CUDA，其次 MPS，否则 CPU\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() \\\n",
    "    else \"mps\" if torch.backends.mps.is_available() \\\n",
    "    else \"cpu\"\n",
    "# 如果是 MPS 设备，则开启后备机制以兼容更多操作\n",
    "if torch_device == \"mps\":\n",
    "    os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f150dd9e",
   "metadata": {},
   "source": [
    "## 加载模型\n",
    "\n",
    "以下代码（以及下一节的代码）修改自 [Hugging Face 示例笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb)。\n",
    "\n",
    "它将下载并设置我们将使用的相关模型和组件。现在先直接运行，然后移动到下一节，确认一切正常后再深入学习细节。\n",
    "\n",
    "如果你已经加载了 pipeline，也可以通过 `pipe.unet`、`pipe.vae` 等方式访问其中的组件。\n",
    "\n",
    "在本笔记本中，我们并未使用任何显存优化技巧——如果你的 GPU 显存不足，可以查看 pipeline 代码，参考诸如 **attention slicing**、切换到半精度（fp16）、将 VAE 留在 CPU 等方法来节省内存。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1572ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载用于将潜变量解码为图像空间的自编码器模型\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    \"CompVis/stable-diffusion-v1-4\",\n",
    "    subfolder=\"vae\"\n",
    ")\n",
    "\n",
    "# 加载分词器和文本编码器，用于将文本进行分词和编码\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "# 加载用于生成潜变量的 UNet 条件模型\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    \"CompVis/stable-diffusion-v1-4\",\n",
    "    subfolder=\"unet\"\n",
    ")\n",
    "\n",
    "# 配置噪声调度器，使用 LMS 离散调度器\n",
    "scheduler = LMSDiscreteScheduler(\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    num_train_timesteps=1000\n",
    ")\n",
    "\n",
    "# 将模型移动到 GPU（或指定的设备）\n",
    "vae = vae.to(torch_device)\n",
    "text_encoder = text_encoder.to(torch_device)\n",
    "unet = unet.to(torch_device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5cbfb8",
   "metadata": {},
   "source": [
    "# TODO 待完成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324842f5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
